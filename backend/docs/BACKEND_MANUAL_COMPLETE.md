# KEEN å¾Œç«¯æ¶æ§‹å®Œæ•´ä½¿ç”¨æ‰‹å†Š / KEEN Backend Architecture Complete Manual

**ä½œè€… / Author**: Odindino  
**æ—¥æœŸ / Date**: 2025-06-07  
**ç‰ˆæœ¬ / Version**: 2.0 Complete Edition

## ç›®éŒ„ / Table of Contents

1. [æ¦‚è¿° / Overview](#æ¦‚è¿°--overview)
2. [æª”æ¡ˆçµæ§‹ / File Structure](#æª”æ¡ˆçµæ§‹--file-structure)
3. [æ¶æ§‹è¨­è¨ˆ / Architecture Design](#æ¶æ§‹è¨­è¨ˆ--architecture-design)
4. [å®Œæ•´ API åƒè€ƒ / Complete API Reference](#å®Œæ•´-api-åƒè€ƒ--complete-api-reference)
5. [æ ¸å¿ƒçµ„ä»¶è©³è§£ / Core Components Details](#æ ¸å¿ƒçµ„ä»¶è©³è§£--core-components-details)
6. [ä½¿ç”¨æŒ‡å— / Usage Guide](#ä½¿ç”¨æŒ‡å—--usage-guide)
7. [é€²éšåŠŸèƒ½ / Advanced Features](#é€²éšåŠŸèƒ½--advanced-features)
8. [ç¯„ä¾‹ç¨‹å¼ç¢¼ / Example Code](#ç¯„ä¾‹ç¨‹å¼ç¢¼--example-code)
9. [ç–‘é›£æ’è§£ / Troubleshooting](#ç–‘é›£æ’è§£--troubleshooting)
10. [æ•ˆèƒ½æœ€ä½³åŒ– / Performance Optimization](#æ•ˆèƒ½æœ€ä½³åŒ–--performance-optimization)

## æ¦‚è¿° / Overview

KEEN å¾Œç«¯æ¡ç”¨å…¨æ–°çš„ã€Œ**é¡å‹ç®¡ç†å™¨ + æª”æ¡ˆä»£ç†**ã€æ¶æ§‹ï¼ˆType Manager + File Proxyï¼‰ï¼Œæä¾›çµ±ä¸€ã€é«˜æ•ˆä¸”æ“´å±•æ€§å¼·çš„ SPM æ•¸æ“šè™•ç†ç³»çµ±ã€‚æ­¤æ¶æ§‹æ”¯æ´å¤šç¨®æª”æ¡ˆæ ¼å¼ï¼ˆTXTã€INTã€DATï¼‰ï¼Œä¸¦æä¾›å®Œæ•´çš„æ•¸æ“šåˆ†æåŠŸèƒ½ã€‚

### ä¸»è¦ç‰¹é» / Key Features

- ğŸ”§ **çµ±ä¸€çš„æ•¸æ“šæ¨¡å‹** - æ¨™æº–åŒ–çš„æ•¸æ“šçµæ§‹å’Œä»‹é¢
- âš¡ **æ™ºèƒ½å¿«å–ç®¡ç†** - LRU å¿«å–æ©Ÿåˆ¶æå‡æ€§èƒ½
- ğŸ¯ **é¡å‹å®‰å…¨** - å®Œæ•´çš„å‹åˆ¥æç¤ºå’Œé©—è­‰
- ğŸ”„ **éˆæ´»çš„åˆ†æå™¨** - å¯æ’æ‹”çš„åˆ†ææ¨¡çµ„ç³»çµ±
- ğŸ“Š **Plotly è¦–è¦ºåŒ–** - å…§å»ºé«˜å“è³ªåœ–è¡¨ç”Ÿæˆ
- ğŸ”— **éˆå¼æ“ä½œ** - ç›´è§€çš„ API è¨­è¨ˆ
- ğŸ’¾ **è¨˜æ†¶é«”æœ€ä½³åŒ–** - æ™ºèƒ½è¼‰å…¥å’Œå¸è¼‰æ©Ÿåˆ¶
- ğŸ” **è©³ç´°æ—¥èªŒ** - å®Œæ•´çš„æ“ä½œè¿½è¹¤

## æª”æ¡ˆçµæ§‹ / File Structure

```
backend/
â”œâ”€â”€ core/                           # æ ¸å¿ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ experiment_session.py       # å¯¦é©—æœƒè©±ï¼ˆä¸»å…¥å£ï¼‰
â”‚   â”œâ”€â”€ file_proxy.py              # æª”æ¡ˆä»£ç†
â”‚   â”œâ”€â”€ type_managers.py           # é¡å‹ç®¡ç†å™¨
â”‚   â”œâ”€â”€ data_models.py             # è³‡æ–™æ¨¡å‹å®šç¾©
â”‚   â”‚
â”‚   â”œâ”€â”€ parsers/                   # è§£æå™¨æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ txt_parser.py          # TXT æª”æ¡ˆè§£æå™¨
â”‚   â”‚   â”œâ”€â”€ int_parser.py          # INT æª”æ¡ˆè§£æå™¨
â”‚   â”‚   â””â”€â”€ dat_parser.py          # DAT æª”æ¡ˆè§£æå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ analyzers/                 # åˆ†æå™¨æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_analyzer.py       # åˆ†æå™¨åŸºé¡
â”‚   â”‚   â”œâ”€â”€ txt_analyzer.py        # TXT åˆ†æå™¨
â”‚   â”‚   â”œâ”€â”€ int_analyzer.py        # INT åˆ†æå™¨
â”‚   â”‚   â”œâ”€â”€ cits_analyzer.py       # CITS åˆ†æå™¨
â”‚   â”‚   â””â”€â”€ dat_analyzer.py        # DAT åˆ†æå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                  # åˆ†æç®—æ³•æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cits_analysis.py       # CITS åˆ†æç®—æ³•
â”‚   â”‚   â”œâ”€â”€ int_analysis.py        # INT åˆ†æç®—æ³•
â”‚   â”‚   â””â”€â”€ profile_analysis.py    # å‰–é¢åˆ†æç®—æ³•
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/             # è¦–è¦ºåŒ–æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spm_plots.py          # SPM åœ–è¡¨
â”‚   â”‚   â””â”€â”€ spectroscopy_plots.py  # å…‰è­œåœ–è¡¨
â”‚   â”‚
â”‚   â”œâ”€â”€ mathematics/               # æ•¸å­¸å·¥å…·æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ geometry.py           # å¹¾ä½•è¨ˆç®—
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # å·¥å…·æ¨¡çµ„
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ algorithms.py         # ç®—æ³•å·¥å…·
â”‚
â”œâ”€â”€ test/                          # æ¸¬è©¦æ¨¡çµ„
â”‚   â”œâ”€â”€ interactive_new_architecture_test.ipynb  # äº¤äº’å¼æ¸¬è©¦
â”‚   â””â”€â”€ unit/                      # å–®å…ƒæ¸¬è©¦
â”‚
â”œâ”€â”€ main.py                        # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ api_mvp.py                     # API å±¤
â”œâ”€â”€ requirements.txt               # ä¾è³´æ¸…å–®
â”œâ”€â”€ BACKEND_MANUAL.md             # åŸºæœ¬ä½¿ç”¨æ‰‹å†Š
â”œâ”€â”€ BACKEND_MANUAL_COMPLETE.md    # å®Œæ•´ä½¿ç”¨æ‰‹å†Šï¼ˆæœ¬æ–‡ä»¶ï¼‰
â”œâ”€â”€ architecture_diagram.py       # æ¶æ§‹åœ–ç”Ÿæˆå™¨
â”œâ”€â”€ architecture_relationship_diagram.png  # æ¶æ§‹é—œè¯åœ–
â””â”€â”€ data_flow_diagram.png         # æ•¸æ“šæµç¨‹åœ–
```

## æ¶æ§‹è¨­è¨ˆ / Architecture Design

### æ ¸å¿ƒæ¶æ§‹æ¨¡å¼ / Core Architecture Pattern

```
ExperimentSession (æœƒè©±ç®¡ç†)
â”œâ”€â”€ TxtManager (TXT æª”æ¡ˆç®¡ç†)      â”€â”€â†’  TxtAnalyzer
â”œâ”€â”€ TopoManager (æ‹“æ’²åœ–ç®¡ç†)       â”€â”€â†’  IntAnalyzer  
â”œâ”€â”€ CitsManager (CITS æ•¸æ“šç®¡ç†)    â”€â”€â†’  CitsAnalyzer
â””â”€â”€ StsManager (STS æ•¸æ“šç®¡ç†)      â”€â”€â†’  DatAnalyzer

FileProxy (æª”æ¡ˆä»£ç†) â”€â”€â†’ çµ±ä¸€å­˜å–ä»‹é¢

DataModels (è³‡æ–™æ¨¡å‹)
â”œâ”€â”€ ParseResult    (è§£æçµæœ)
â”œâ”€â”€ FileInfo       (æª”æ¡ˆè³‡è¨Š)
â”œâ”€â”€ TopoData       (æ‹“æ’²åœ–è³‡æ–™)
â”œâ”€â”€ CitsData       (CITS è³‡æ–™)
â”œâ”€â”€ StsData        (STS è³‡æ–™)
â””â”€â”€ TxtData        (TXT è³‡æ–™)
```

### æ•¸æ“šæµç¨‹ / Data Flow

```
åŸå§‹æª”æ¡ˆ           è§£æå™¨           æ¨™æº–æ•¸æ“š           é¡å‹ç®¡ç†å™¨         æª”æ¡ˆä»£ç†          åˆ†æå™¨            åˆ†æçµæœ
Raw Files    â†’    Parsers    â†’    StandardData  â†’   TypeManager   â†’   FileProxy   â†’   Analyzers   â†’   Results
(.txt/.int/.dat)  (å„ç¨®Parser)     (ParseResult)     (å¿«å–+ç®¡ç†)       (çµ±ä¸€ä»‹é¢)      (åˆ†æåŠŸèƒ½)        (å«åœ–è¡¨)
```

## å®Œæ•´ API åƒè€ƒ / Complete API Reference

### 1. ExperimentSession é¡åˆ¥

**æª”æ¡ˆä½ç½®**: `backend/core/experiment_session.py`

**é¡åˆ¥æè¿°**: å¯¦é©—æœƒè©±é¡åˆ¥ï¼Œæ•´åˆæ‰€æœ‰è³‡æ–™ç®¡ç†åŠŸèƒ½çš„ä¸»è¦å…¥å£é»

#### åˆå§‹åŒ–åƒæ•¸
```python
ExperimentSession(txt_file_path: str = None, cache_size: int = 20)
```
- `txt_file_path` (str, optional): TXT æª”æ¡ˆè·¯å¾‘
- `cache_size` (int, é è¨­=20): å¿«å–å¤§å°

#### å…¬å…±å±¬æ€§
```python
# é¡å‹ç®¡ç†å™¨
session.txt: TxtManager           # TXT æª”æ¡ˆç®¡ç†å™¨
session.topo: TopoManager         # æ‹“æ’²åœ–ç®¡ç†å™¨
session.cits: CitsManager         # CITS è³‡æ–™ç®¡ç†å™¨
session.sts: StsManager           # STS è³‡æ–™ç®¡ç†å™¨

# åŸºæœ¬è³‡è¨Š
session.txt_file_path: Path       # TXT æª”æ¡ˆè·¯å¾‘
session.experiment_name: str      # å¯¦é©—åç¨±
session.creation_time: datetime   # å‰µå»ºæ™‚é–“
```

#### ä¸»è¦å…¬å…±æ–¹æ³•

##### æª”æ¡ˆæ“ä½œ
```python
def get_file_proxy(file_key: str) -> FileProxy:
    """ç²å–æª”æ¡ˆä»£ç†"""

def has_file(file_key: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦æœ‰æŒ‡å®šæª”æ¡ˆ"""

def load_multiple_files(file_keys: List[str]) -> Dict[str, ParseResult]:
    """æ‰¹æ¬¡è¼‰å…¥å¤šå€‹æª”æ¡ˆ"""

def unload_all_files(exclude_txt: bool = True) -> int:
    """å¸è¼‰æ‰€æœ‰æª”æ¡ˆ"""
```

##### è³‡è¨Šç²å–
```python
def get_memory_info() -> Dict[str, Any]:
    """ç²å–è¨˜æ†¶é«”ä½¿ç”¨è³‡è¨Š"""

def get_session_summary() -> Dict[str, Any]:
    """ç²å–æœƒè©±æ‘˜è¦"""

def clear_all_caches() -> None:
    """æ¸…ç†æ‰€æœ‰å¿«å–"""
```

#### å±¬æ€§æ–¹æ³• (Properties)
```python
@property
def available_files(self) -> Dict[str, List[str]]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æª”æ¡ˆ"""

@property  
def loaded_files(self) -> Dict[str, List[str]]:
    """åˆ—å‡ºæ‰€æœ‰å·²è¼‰å…¥æª”æ¡ˆ"""

@property
def scan_parameters(self) -> ScanParameters:
    """ç²å–æƒæåƒæ•¸"""

@property
def experiment_info(self) -> Dict[str, Any]:
    """ç²å–å¯¦é©—è³‡è¨Š"""
```

#### ä¾¿åˆ©æ–¹æ³•
```python
def get_topo_files() -> List[str]:
    """ç²å–æ‰€æœ‰æ‹“æ’²æª”æ¡ˆéµå€¼"""

def get_cits_files() -> List[str]:
    """ç²å–æ‰€æœ‰ CITS æª”æ¡ˆéµå€¼"""

def get_sts_files() -> List[str]:
    """ç²å–æ‰€æœ‰ STS æª”æ¡ˆéµå€¼"""

def find_files_by_signal_type(signal_type: str) -> List[str]:
    """æ ¹æ“šè¨Šè™Ÿé¡å‹å°‹æ‰¾æª”æ¡ˆ"""

def find_files_by_direction(direction: str) -> List[str]:
    """æ ¹æ“šæƒææ–¹å‘å°‹æ‰¾æª”æ¡ˆ"""
```

### 2. FileProxy é¡åˆ¥

**æª”æ¡ˆä½ç½®**: `backend/core/file_proxy.py`

**é¡åˆ¥æè¿°**: æª”æ¡ˆä»£ç†é¡åˆ¥ï¼Œæä¾›ç›´è§€çš„æª”æ¡ˆå­˜å–ä»‹é¢

#### åˆå§‹åŒ–åƒæ•¸
```python
FileProxy(session: ExperimentSession, file_key: str)
```

#### ä¸»è¦å…¬å…±å±¬æ€§
```python
# æ ¸å¿ƒå±¬æ€§
proxy.data: SPMData              # ç²å–æª”æ¡ˆæ•¸æ“š
proxy.metadata: Dict[str, Any]   # ç²å–æª”æ¡ˆå…ƒè³‡æ–™
proxy.analyzer                   # ç²å–åˆ†æå™¨å¯¦ä¾‹
proxy.file_info: FileInfo        # ç²å–æª”æ¡ˆè³‡è¨Š
proxy.is_loaded: bool            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²è¼‰å…¥
proxy.file_type: str             # ç²å–æª”æ¡ˆé¡å‹
```

#### æ‹“æ’²åœ–å°ˆç”¨å±¬æ€§
```python
# å¿«æ·å­˜å–æ‹“æ’²åœ–å±¬æ€§
proxy.image: Optional[np.ndarray]      # åœ–åƒè³‡æ–™
proxy.flattened: Optional[np.ndarray]  # å¹³å¦åŒ–åœ–åƒ
proxy.x_range: Optional[float]         # X æ–¹å‘ç¯„åœ
proxy.y_range: Optional[float]         # Y æ–¹å‘ç¯„åœ  
proxy.shape: Optional[tuple]           # æ•¸æ“šå½¢ç‹€
```

#### CITS å°ˆç”¨å±¬æ€§
```python
# å¿«æ·å­˜å– CITS å±¬æ€§
proxy.bias_values: Optional[np.ndarray]  # åå£“å€¼
proxy.data_3d: Optional[np.ndarray]      # 3D æ•¸æ“š
```

#### ä¸»è¦å…¬å…±æ–¹æ³•
```python
def get_bias_slice(bias_index: int) -> Optional[np.ndarray]:
    """ç²å–ç‰¹å®šåå£“çš„ 2D åˆ‡ç‰‡"""

def flatten_plane(method: str = 'linear', **kwargs) -> Optional[np.ndarray]:
    """åŸ·è¡Œå¹³é¢å¹³å¦åŒ–"""

def extract_profile(x1: float, y1: float, x2: float, y2: float, 
                   name: str = "profile") -> Optional[ProfileLine]:
    """æå–å‰–é¢ç·š"""

def extract_iv_curve(x: int, y: int) -> Optional[Dict[str, Any]]:
    """æå– I-V æ›²ç·š"""

def reload(force: bool = True) -> 'FileProxy':
    """é‡æ–°è¼‰å…¥æª”æ¡ˆ"""

def unload() -> bool:
    """å¸è¼‰æª”æ¡ˆ"""

def get_analysis_history() -> list:
    """ç²å–åˆ†ææ­·å²"""
```

### 3. TypeManager åŸºé¡åŠå…¶å­é¡

**æª”æ¡ˆä½ç½®**: `backend/core/type_managers.py`

#### TypeManager (æŠ½è±¡åŸºé¡)

##### åˆå§‹åŒ–åƒæ•¸
```python
TypeManager(cache_size: int = 20, session = None)
```

##### ä¸»è¦å…¬å…±æ–¹æ³•
```python
def add_file(key: str, info: FileInfo) -> None:
    """æ·»åŠ æª”æ¡ˆè³‡è¨Š"""

def has_file(key: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦æœ‰æŒ‡å®šçš„æª”æ¡ˆ"""

def is_loaded(key: str) -> bool:
    """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²è¼‰å…¥"""

def load(key: str, force_reload: bool = False) -> ParseResult:
    """è¼‰å…¥æª”æ¡ˆ"""

def get_analyzer(key: str):
    """ç²å–æˆ–å‰µå»ºåˆ†æå™¨"""

def unload(key: str) -> bool:
    """å¸è¼‰æª”æ¡ˆ"""

def get_files() -> Dict[str, FileInfo]:
    """ç²å–æ‰€æœ‰æª”æ¡ˆè³‡è¨Š"""

def get_loaded_files() -> List[str]:
    """ç²å–å·²è¼‰å…¥çš„æª”æ¡ˆåˆ—è¡¨"""

def get_cache_info() -> Dict[str, Any]:
    """ç²å–å¿«å–è³‡è¨Š"""

def clear_cache() -> None:
    """æ¸…ç†å¿«å–"""
```

##### æŠ½è±¡æ–¹æ³•
```python
@abstractmethod
def _parse_file(info: FileInfo) -> ParseResult:
    """è§£ææª”æ¡ˆï¼ˆå­é¡å¯¦ä½œï¼‰"""

@abstractmethod  
def _create_analyzer(key: str):
    """å‰µå»ºåˆ†æå™¨ï¼ˆå­é¡å¯¦ä½œï¼‰"""
```

#### TxtManager / TopoManager / CitsManager / StsManager

é€™äº›å­é¡ç¹¼æ‰¿ TypeManagerï¼Œæä¾›ç‰¹å®šæª”æ¡ˆé¡å‹çš„ç®¡ç†åŠŸèƒ½ã€‚

### 4. è³‡æ–™æ¨¡å‹ (data_models.py)

**æª”æ¡ˆä½ç½®**: `backend/core/data_models.py`

#### ParseResult
```python
@dataclass
class ParseResult:
    metadata: Dict[str, Any]      # å…ƒè³‡æ–™
    data: Any                     # ä¸»è¦è³‡æ–™
    derived: Dict[str, Any]       # è¡ç”Ÿè³‡æ–™
    errors: List[str]             # éŒ¯èª¤è¨Šæ¯
    warnings: List[str]           # è­¦å‘Šè¨Šæ¯
    timestamp: datetime           # æ™‚é–“æˆ³
    parser_type: str              # è§£æå™¨é¡å‹
    
    @property
    def success(self) -> bool:
        """æ˜¯å¦æˆåŠŸè§£æ"""
    
    def add_error(message: str) -> None:
        """æ·»åŠ éŒ¯èª¤è¨Šæ¯"""
    
    def add_warning(message: str) -> None:
        """æ·»åŠ è­¦å‘Šè¨Šæ¯"""
```

#### FileInfo
```python
@dataclass
class FileInfo:
    path: str                           # æª”æ¡ˆè·¯å¾‘
    type: str                          # æª”æ¡ˆé¡å‹
    size: int                          # æª”æ¡ˆå¤§å°
    signal_type: Optional[str] = None  # è¨Šè™Ÿé¡å‹
    direction: Optional[str] = None    # æƒææ–¹å‘
    loaded: bool = False               # æ˜¯å¦å·²è¼‰å…¥
    loaded_at: Optional[datetime] = None  # è¼‰å…¥æ™‚é–“
    
    @property
    def filename(self) -> str:
        """ç²å–æª”æ¡ˆåç¨±"""
    
    @property
    def human_readable_size(self) -> str:
        """äººé¡å¯è®€çš„æª”æ¡ˆå¤§å°"""
```

#### ScanParameters
```python
@dataclass
class ScanParameters:
    x_pixel: int                    # X æ–¹å‘åƒç´ æ•¸
    y_pixel: int                    # Y æ–¹å‘åƒç´ æ•¸
    x_range: float                  # X æ–¹å‘æƒæç¯„åœ nm
    y_range: float                  # Y æ–¹å‘æƒæç¯„åœ nm
    
    @property
    def pixel_scale_x(self) -> float:
        """X æ–¹å‘åƒç´ å°ºåº¦ nm/pixel"""
    
    @property
    def pixel_scale_y(self) -> float:
        """Y æ–¹å‘åƒç´ å°ºåº¦ nm/pixel"""
    
    @property
    def aspect_ratio(self) -> float:
        """é•·å¯¬æ¯”"""
    
    @property
    def total_pixels(self) -> int:
        """ç¸½åƒç´ æ•¸"""
```

#### TopoData
```python
@dataclass 
class TopoData:
    image: np.ndarray                    # åŸå§‹åœ–åƒæ•¸æ“š
    x_range: float                       # X æ–¹å‘ç¯„åœ nm
    y_range: float                       # Y æ–¹å‘ç¯„åœ nm
    x_pixels: int                        # X æ–¹å‘åƒç´ æ•¸
    y_pixels: int                        # Y æ–¹å‘åƒç´ æ•¸
    data_scale: float                    # æ•¸æ“š scale
    signal_type: str = "Topo"           # è¨Šè™Ÿé¡å‹
    direction: Optional[str] = None      # æƒææ–¹å‘
    
    # è™•ç†å¾Œçš„è³‡æ–™
    flattened: Optional[np.ndarray] = None
    profile_lines: Dict[str, Any] = field(default_factory=dict)
    statistics: Dict[str, float] = field(default_factory=dict)
    
    @property
    def shape(self) -> tuple:
        """åœ–åƒå½¢ç‹€"""
    
    @property
    def pixel_scale_x(self) -> float:
        """X æ–¹å‘åƒç´ å°ºåº¦"""
    
    @property
    def pixel_scale_y(self) -> float:
        """Y æ–¹å‘åƒç´ å°ºåº¦"""
    
    @property
    def current_image(self) -> np.ndarray:
        """ç•¶å‰é¡¯ç¤ºçš„åœ–åƒï¼ˆå¹³å¦åŒ–å„ªå…ˆï¼‰"""
```

#### CitsData
```python
@dataclass
class CitsData:
    data_3d: np.ndarray                      # 3D æ•¸æ“š (n_bias, y, x)
    bias_values: np.ndarray                  # åå£“å€¼
    grid_size: List[int]                     # ç¶²æ ¼å¤§å° [x, y]
    x_range: float                           # X æ–¹å‘ç¯„åœ nm
    y_range: float                           # Y æ–¹å‘ç¯„åœ nm
    measurement_mode: str = "CITS"           # é‡æ¸¬æ¨¡å¼
    
    # åˆ†æçµæœ
    iv_curves: Dict[str, Any] = field(default_factory=dict)
    conductance_maps: Dict[str, np.ndarray] = field(default_factory=dict)
    gap_maps: Dict[str, np.ndarray] = field(default_factory=dict)
    
    @property
    def shape(self) -> tuple:
        """æ•¸æ“šå½¢ç‹€"""
    
    @property
    def n_bias_points(self) -> int:
        """åå£“é»æ•¸"""
    
    @property
    def bias_range(self) -> tuple:
        """åå£“ç¯„åœ"""
    
    def get_bias_slice(bias_index: int) -> np.ndarray:
        """ç²å–ç‰¹å®šåå£“çš„2Dåˆ‡ç‰‡"""
```

#### StsData
```python
@dataclass
class StsData:
    data_2d: np.ndarray                      # 2D æ•¸æ“š (n_bias, n_points)
    bias_values: np.ndarray                  # åå£“å€¼
    x_coords: np.ndarray                     # X åº§æ¨™
    y_coords: np.ndarray                     # Y åº§æ¨™
    measurement_mode: str = "STS"            # é‡æ¸¬æ¨¡å¼
    
    # åˆ†æçµæœ
    gap_values: Dict[str, float] = field(default_factory=dict)
    peak_positions: Dict[str, List[float]] = field(default_factory=dict)
    
    @property
    def shape(self) -> tuple:
        """æ•¸æ“šå½¢ç‹€"""
    
    @property
    def n_points(self) -> int:
        """é‡æ¸¬é»æ•¸"""
    
    @property
    def n_bias_points(self) -> int:
        """åå£“é»æ•¸"""
```

#### TxtData
```python
@dataclass
class TxtData:
    experiment_info: Dict[str, Any]          # å¯¦é©—è³‡è¨Š
    scan_parameters: ScanParameters          # æƒæåƒæ•¸
    int_files: List[Dict[str, Any]]         # INT æª”æ¡ˆåˆ—è¡¨
    dat_files: List[Dict[str, Any]]         # DAT æª”æ¡ˆåˆ—è¡¨
    signal_types: List[str]                  # è¨Šè™Ÿé¡å‹åˆ—è¡¨
    
    @property
    def experiment_name(self) -> str:
        """å¯¦é©—åç¨±"""
    
    @property
    def total_files(self) -> int:
        """ç¸½æª”æ¡ˆæ•¸"""
```

### 5. è§£æå™¨é¡åˆ¥ (Parsers)

#### TxtParser
```python
class TxtParser:
    def __init__(file_path: str):
        """åˆå§‹åŒ– TXT è§£æå™¨"""
    
    def parse() -> ParseResult:
        """è§£æ TXT æª”æ¡ˆ"""
    
    def get_int_files():
        """è¿”å› INT æª”æ¡ˆæè¿°åˆ—è¡¨"""
    
    def get_dat_files():
        """è¿”å› DAT æª”æ¡ˆæè¿°åˆ—è¡¨"""
```

#### IntParser
```python
class IntParser:
    def __init__(file_path: str, scale: float, x_pixel: int, y_pixel: int):
        """åˆå§‹åŒ– INT è§£æå™¨"""
    
    def parse() -> ParseResult:
        """è§£æ INT æª”æ¡ˆä¸¦è¿”å›å½¢è²Œæ•¸æ“š"""
```

#### DatParser
```python
class DatParser:
    def __init__():
        """åˆå§‹åŒ– DAT è§£æå™¨"""
    
    def parse(file_path: str, dat_info: dict = None) -> ParseResult:
        """è§£æ DAT æª”æ¡ˆ"""
    
    @staticmethod
    def prepare_cits_for_display(data_3d: np.ndarray, scan_direction: str) -> np.ndarray:
        """ç‚ºé¡¯ç¤ºæº–å‚™ CITS æ•¸æ“šæ–¹å‘"""
    
    @staticmethod
    def is_cits_data(data: Dict) -> bool:
        """æª¢æŸ¥è§£æå¾Œçš„æ•¸æ“šæ˜¯å¦ç‚º CITS æ ¼å¼"""
```

### 6. åˆ†æå™¨é¡åˆ¥ (Analyzers)

#### BaseAnalyzer (æŠ½è±¡åŸºé¡)
```python
class BaseAnalyzer(ABC):
    def __init__(data: SPMData):
        """åˆå§‹åŒ–åˆ†æå™¨"""
    
    # ä¸»è¦å±¬æ€§
    data: SPMData                    # SPM æ•¸æ“š
    state: AnalysisState            # åˆ†æç‹€æ…‹
    analysis_history: List[Dict]    # åˆ†ææ­·å²
    cached_results: Dict[str, Any]  # å¿«å–æ•¸æ“š
    
    @abstractmethod
    def analyze(**kwargs) -> Dict[str, Any]:
        """æ ¸å¿ƒåˆ†ææ–¹æ³•ï¼ˆæŠ½è±¡ï¼‰"""
    
    def get_results() -> Dict[str, Any]:
        """ç²å–æœ€æ–°çš„åˆ†æçµæœ"""
    
    def get_history() -> List[Dict]:
        """ç²å–åˆ†ææ­·å²"""
    
    def clear_cache() -> None:
        """æ¸…ç†å¿«å–æ•¸æ“š"""
    
    def get_cache_info() -> Dict[str, Any]:
        """ç²å–å¿«å–ä¿¡æ¯"""
    
    def get_status() -> Dict[str, Any]:
        """ç²å–åˆ†æå™¨ç‹€æ…‹"""
    
    def reset() -> None:
        """é‡ç½®åˆ†æå™¨ç‹€æ…‹"""
    
    def validate_input(**kwargs) -> bool:
        """é©—è­‰è¼¸å…¥æ•¸æ“š"""
```

#### IntAnalyzer
```python
class IntAnalyzer(BaseAnalyzer):
    def analyze(**kwargs) -> Dict[str, Any]:
        """åˆ†æ INT å½¢è²Œæ•¸æ“š"""
    
    def apply_flattening(method: str = 'linewise_mean', **kwargs) -> Dict[str, Any]:
        """æ‡‰ç”¨å¹³é¢åŒ–è™•ç†"""
    
    def apply_tilt_correction(direction: str, step_size: int = 10, 
                             fine_tune: bool = False) -> Dict[str, Any]:
        """æ‡‰ç”¨å‚¾æ–œæ ¡æ­£"""
    
    def extract_line_profile(start_point: Tuple[int, int], 
                           end_point: Tuple[int, int], 
                           method: str = 'interpolation') -> Dict[str, Any]:
        """æå–ç·šæ®µå‰–é¢"""
    
    def detect_features(feature_type: str = 'peaks', **kwargs) -> Dict[str, Any]:
        """æª¢æ¸¬è¡¨é¢ç‰¹å¾µ"""
    
    def reset_to_original() -> Dict[str, Any]:
        """é‡ç½®åˆ°åŸå§‹æ•¸æ“š"""
```

#### CitsAnalyzer
```python
class CitsAnalyzer(BaseAnalyzer):
    def analyze(**kwargs) -> Dict[str, Any]:
        """åˆ†æ CITS æ•¸æ“š"""
    
    def extract_line_profile(start_coord: Tuple[int, int], 
                           end_coord: Tuple[int, int],
                           sampling_method: str = 'bresenham') -> Dict[str, Any]:
        """æå–ç·šæ®µå‰–é¢å…‰è­œ"""
    
    def get_bias_slice(bias_index: int) -> Dict[str, Any]:
        """ç²å–ç‰¹å®šåå£“çš„2Dåˆ‡ç‰‡"""
    
    def analyze_conductance_maps(**kwargs) -> Dict[str, Any]:
        """åˆ†æé›»å°åœ–"""
```

## æ ¸å¿ƒçµ„ä»¶è©³è§£ / Core Components Details

### å¯¦é©—æœƒè©± (ExperimentSession)

å¯¦é©—æœƒè©±æ˜¯æ•´å€‹ç³»çµ±çš„æ ¸å¿ƒï¼Œæä¾›çµ±ä¸€çš„å¯¦é©—ç®¡ç†ä»‹é¢ã€‚

#### å…§éƒ¨æ¶æ§‹
```python
session = ExperimentSession("/path/to/experiment.txt")

# è‡ªå‹•è¼‰å…¥ TXT æª”æ¡ˆä¸¦ç™¼ç¾ç›¸é—œæª”æ¡ˆ
session._load_experiment()

# å››å€‹ç®¡ç†å™¨è‡ªå‹•åˆå§‹åŒ–
session.txt    # TxtManager
session.topo   # TopoManager  
session.cits   # CitsManager
session.sts    # StsManager
```

#### æª”æ¡ˆç™¼ç¾æ©Ÿåˆ¶
```python
# TXT è§£æå¾Œè‡ªå‹•è¨»å†Šç›¸é—œæª”æ¡ˆ
txt_data = session.txt.load("main_txt")
session._register_associated_files(txt_data.data)

# è‡ªå‹•åˆ†é¡æª”æ¡ˆé¡å‹
for int_file in txt_data.data.int_files:
    session.topo.add_file(key, file_info)
    
for dat_file in txt_data.data.dat_files:
    if is_cits(dat_file):
        session.cits.add_file(key, file_info)
    else:
        session.sts.add_file(key, file_info)
```

### æª”æ¡ˆä»£ç† (FileProxy)

æª”æ¡ˆä»£ç†æä¾›çµ±ä¸€ä¸”ç›´è§€çš„æª”æ¡ˆå­˜å–ä»‹é¢ã€‚

#### æ™ºèƒ½å±¬æ€§å­˜å–
```python
proxy = session.get_file_proxy("TopoFwd.int")

# ç›´æ¥å­˜å–æ•¸æ“šå±¬æ€§
image = proxy.image           # ç­‰åŒæ–¼ proxy.data.image
shape = proxy.shape          # ç­‰åŒæ–¼ proxy.data.shape
x_range = proxy.x_range      # ç­‰åŒæ–¼ proxy.data.x_range

# æ™ºèƒ½é¡å‹åˆ¤æ–·
if proxy.file_type == "topo":
    # æä¾›æ‹“æ’²åœ–å°ˆç”¨æ–¹æ³•
    flattened = proxy.flatten_plane()
    
elif proxy.file_type == "cits":
    # æä¾› CITS å°ˆç”¨æ–¹æ³•
    slice_data = proxy.get_bias_slice(100)
```

#### åˆ†æå™¨æ•´åˆ
```python
# å»¶é²è¼‰å…¥åˆ†æå™¨
analyzer = proxy.analyzer     # ç¬¬ä¸€æ¬¡å­˜å–æ™‚å‰µå»º

# åŸ·è¡Œåˆ†æ
result = analyzer.analyze()

# ç²å–åˆ†ææ­·å²
history = proxy.get_analysis_history()
```

### é¡å‹ç®¡ç†å™¨ (TypeManager)

é¡å‹ç®¡ç†å™¨æä¾›æª”æ¡ˆçš„è¼‰å…¥ã€å¿«å–å’Œåˆ†æå™¨ç®¡ç†ã€‚

#### LRU å¿«å–æ©Ÿåˆ¶
```python
manager = session.topo

# è¼‰å…¥æª”æ¡ˆï¼ˆåŠ å…¥å¿«å–ï¼‰
result1 = manager.load("file1")   # å¿«å– miss
result2 = manager.load("file1")   # å¿«å– hit

# æŸ¥çœ‹å¿«å–ç‹€æ…‹
cache_info = manager.get_cache_info()
print(f"å‘½ä¸­ç‡: {cache_info['hit_rate']:.2%}")

# å¿«å–æ»¿æ™‚è‡ªå‹•ç§»é™¤ LRU é …ç›®
for i in range(25):  # è¶…éé è¨­å¿«å–å¤§å° 20
    manager.load(f"file{i}")
```

#### åˆ†æå™¨å·¥å» 
```python
# æ¯å€‹æª”æ¡ˆå°æ‡‰ä¸€å€‹åˆ†æå™¨å¯¦ä¾‹
analyzer1 = manager.get_analyzer("file1")
analyzer2 = manager.get_analyzer("file2")

# åˆ†æå™¨èˆ‡æª”æ¡ˆç”Ÿå‘½é€±æœŸç¶å®š
manager.unload("file1")  # åŒæ™‚ç§»é™¤åˆ†æå™¨
```

## ä½¿ç”¨æŒ‡å— / Usage Guide

### åŸºæœ¬å·¥ä½œæµç¨‹ / Basic Workflow

#### 1. åˆå§‹åŒ–å’Œè¼‰å…¥
```python
from backend.core.experiment_session import ExperimentSession

# åˆå§‹åŒ–æœƒè©±
session = ExperimentSession()

# è¼‰å…¥å¯¦é©—æª”æ¡ˆ
session.load_txt_file("/path/to/experiment.txt")

# æª¢æŸ¥è¼‰å…¥ç‹€æ…‹
print("å¯ç”¨æª”æ¡ˆ:", session.available_files)
print("å·²è¼‰å…¥æª”æ¡ˆ:", session.loaded_files)
```

#### 2. æ‹“æ’²åœ–åˆ†æå®Œæ•´æµç¨‹
```python
# ç²å–æ‹“æ’²åœ–æª”æ¡ˆ
topo_files = session.get_topo_files()
topo_key = topo_files[0]  # é¸æ“‡ç¬¬ä¸€å€‹

# ç²å–æª”æ¡ˆä»£ç†
topo = session.get_file_proxy(topo_key)

# æª¢æŸ¥åŸºæœ¬è³‡è¨Š
print(f"åœ–åƒå°ºå¯¸: {topo.shape}")
print(f"æƒæç¯„åœ: {topo.x_range} x {topo.y_range} nm")
print(f"è¨Šè™Ÿé¡å‹: {topo.file_info.signal_type}")

# åŸ·è¡ŒåŸºæœ¬åˆ†æ
basic_result = topo.analyzer.analyze()
print("åŸºæœ¬çµ±è¨ˆ:", basic_result['data']['statistics'])

# å¹³å¦åŒ–è™•ç†
flatten_result = topo.analyzer.apply_flattening(
    method='polynomial',  # å¤šé …å¼å¹³å¦åŒ–
    order=2,             # äºŒæ¬¡å¤šé …å¼
    mask_threshold=0.1   # é®ç½©é–¾å€¼
)

# æª¢è¦–å¹³å¦åŒ–æ•ˆæœ
original_std = np.std(topo.image)
flattened_std = np.std(flatten_result['data']['flattened_image'])
print(f"å¹³å¦åŒ–æ”¹å–„: {original_std:.3f} â†’ {flattened_std:.3f}")

# å‚¾æ–œæ ¡æ­£
tilt_result = topo.analyzer.apply_tilt_correction(
    direction='x',       # X æ–¹å‘æ ¡æ­£
    step_size=10,       # æ­¥é•·
    fine_tune=True      # ç²¾ç´°èª¿æ•´
)

# æå–å‰–é¢ç·š
profile_result = topo.analyzer.extract_line_profile(
    start_point=(10, 50),    # èµ·é» (x, y)
    end_point=(200, 50),     # çµ‚é» (x, y)
    method='interpolation'   # æ’å€¼æ–¹æ³•
)

# æª¢è¦–å‰–é¢æ•¸æ“š
profile_data = profile_result['data']['profile_data']
distances = profile_result['data']['distances']
print(f"å‰–é¢é•·åº¦: {profile_result['data']['physical_length']:.2f} nm")

# ç‰¹å¾µæª¢æ¸¬
feature_result = topo.analyzer.detect_features(
    feature_type='peaks',    # æª¢æ¸¬å³°å€¼
    prominence=1.0,         # å³°å€¼çªå‡ºåº¦
    distance=10             # æœ€å°è·é›¢
)

print(f"æª¢æ¸¬åˆ° {len(feature_result['data']['features'])} å€‹ç‰¹å¾µ")

# é¡¯ç¤ºåœ–è¡¨
for plot_name, fig in basic_result['plots'].items():
    fig.show()
```

#### 3. CITS æ•¸æ“šåˆ†æå®Œæ•´æµç¨‹
```python
# ç²å– CITS æª”æ¡ˆ
cits_files = session.get_cits_files()
cits_key = cits_files[0]

# ç²å–æª”æ¡ˆä»£ç†
cits = session.get_file_proxy(cits_key)

# æª¢æŸ¥åŸºæœ¬è³‡è¨Š
print(f"CITS æ•¸æ“šå°ºå¯¸: {cits.shape}")
print(f"åå£“ç¯„åœ: {cits.bias_values.min():.2f} ~ {cits.bias_values.max():.2f} V")
print(f"åå£“é»æ•¸: {len(cits.bias_values)}")

# åŸ·è¡ŒåŸºæœ¬åˆ†æ
basic_result = cits.analyzer.analyze()
bias_pattern = basic_result['data']['bias_pattern']
print(f"åå£“æ¨¡å¼: {bias_pattern['pattern_type']}")

# æå–ç·šæ®µå‰–é¢å…‰è­œ
line_result = cits.analyzer.extract_line_profile(
    start_coord=(20, 20),        # èµ·é» (x, y)
    end_coord=(80, 80),          # çµ‚é» (x, y)
    sampling_method='bresenham'  # æ¡æ¨£æ–¹æ³•
)

line_sts = line_result['data']['line_sts']  # (n_bias, n_points)
print(f"ç·šæ®µå‰–é¢å½¢ç‹€: {line_sts.shape}")

# ç²å–ç‰¹å®šåå£“åˆ‡ç‰‡
bias_index = len(cits.bias_values) // 2  # ä¸­é–“åå£“
slice_result = cits.analyzer.get_bias_slice(bias_index)
slice_data = slice_result['data']['slice_data']
bias_value = slice_result['data']['bias_value']
print(f"åå£“ {bias_value:.3f}V åˆ‡ç‰‡å½¢ç‹€: {slice_data.shape}")

# é›»å°åˆ†æ
conductance_result = cits.analyzer.analyze_conductance_maps(
    smoothing=True,      # å•Ÿç”¨å¹³æ»‘
    method='gradient'    # æ¢¯åº¦æ–¹æ³•
)

# é¡¯ç¤ºæ‰€æœ‰åœ–è¡¨
for plot_name, fig in basic_result['plots'].items():
    fig.show()
    
for plot_name, fig in line_result['plots'].items():
    fig.show()
    
for plot_name, fig in slice_result['plots'].items():
    fig.show()
```

#### 4. æœƒè©±ç®¡ç†å’Œæ•ˆèƒ½ç›£æ§
```python
# æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
memory_info = session.get_memory_info()
print("è¨˜æ†¶é«”ä½¿ç”¨:", memory_info)

# æª¢æŸ¥å¿«å–ç‹€æ…‹
for manager_name in ['txt', 'topo', 'cits', 'sts']:
    manager = getattr(session, manager_name)
    cache_info = manager.get_cache_info()
    print(f"{manager_name} å¿«å–å‘½ä¸­ç‡: {cache_info['hit_rate']:.2%}")

# æ‰¹æ¬¡è¼‰å…¥æª”æ¡ˆ
file_keys = session.get_topo_files()[:5]  # å‰5å€‹æª”æ¡ˆ
results = session.load_multiple_files(file_keys)
print(f"æ‰¹æ¬¡è¼‰å…¥ {len(results)} å€‹æª”æ¡ˆ")

# æ¸…ç†å¿«å–é‡‹æ”¾è¨˜æ†¶é«”
session.clear_all_caches()
print("å¿«å–å·²æ¸…ç†")

# ç²å–æœƒè©±æ‘˜è¦
summary = session.get_session_summary()
print("æœƒè©±æ‘˜è¦:", summary)
```

## é€²éšåŠŸèƒ½ / Advanced Features

### è‡ªè¨‚åˆ†æåƒæ•¸

#### æ‹“æ’²åœ–é€²éšå¹³å¦åŒ–
```python
# ç·šæ€§å¹³å¦åŒ–ï¼ˆé€è¡Œï¼‰
linear_result = topo.analyzer.apply_flattening(
    method='linewise_mean',
    direction='x',           # å¹³å¦åŒ–æ–¹å‘
    exclude_borders=True,    # æ’é™¤é‚Šç•Œ
    border_width=5          # é‚Šç•Œå¯¬åº¦
)

# å¤šé …å¼å¹³å¦åŒ–ï¼ˆæ•´é«”ï¼‰
poly_result = topo.analyzer.apply_flattening(
    method='polynomial',
    order=3,                # ä¸‰æ¬¡å¤šé …å¼
    mask_outliers=True,     # é®ç½©ç•°å¸¸å€¼
    outlier_threshold=3.0   # ç•°å¸¸å€¼é–¾å€¼ï¼ˆæ¨™æº–å·®å€æ•¸ï¼‰
)

# å¹³é¢å¹³å¦åŒ–ï¼ˆä¸‰é»æ³•ï¼‰
plane_result = topo.analyzer.apply_flattening(
    method='plane',
    corners_only=True,      # åªä½¿ç”¨è§’é»
    manual_points=[(10,10), (200,10), (10,200)]  # æ‰‹å‹•æŒ‡å®šé»
)
```

#### CITS é€²éšå…‰è­œåˆ†æ
```python
# é«˜å¯†åº¦ç·šæ®µæ¡æ¨£
dense_line = cits.analyzer.extract_line_profile(
    start_coord=(0, 0),
    end_coord=(99, 99),
    sampling_method='interpolate',  # æ’å€¼æ¡æ¨£
    num_points=200                 # æŒ‡å®šæ¡æ¨£é»æ•¸
)

# å¤šç·šæ®µä¸¦è¡Œåˆ†æ
line_profiles = []
coordinates = [
    ((0, 50), (99, 50)),    # æ°´å¹³ç·š
    ((50, 0), (50, 99)),    # å‚ç›´ç·š
    ((0, 0), (99, 99)),     # å°è§’ç·š
]

for start, end in coordinates:
    result = cits.analyzer.extract_line_profile(start, end)
    line_profiles.append(result)

# åå£“åºåˆ—åˆ†æ
bias_slices = []
for i in range(0, len(cits.bias_values), 50):  # æ¯50å€‹åå£“å–ä¸€å€‹
    slice_result = cits.analyzer.get_bias_slice(i)
    bias_slices.append(slice_result)
```

### æ‰¹æ¬¡è™•ç†å’Œè‡ªå‹•åŒ–

#### æ‰¹æ¬¡æª”æ¡ˆåˆ†æ
```python
def batch_analyze_topo_files(session, signal_type=None, direction=None):
    """æ‰¹æ¬¡åˆ†ææ‹“æ’²æª”æ¡ˆ"""
    results = {}
    
    # ç¯©é¸æª”æ¡ˆ
    if signal_type:
        files = session.find_files_by_signal_type(signal_type)
    elif direction:
        files = session.find_files_by_direction(direction)
    else:
        files = session.get_topo_files()
    
    for file_key in files:
        try:
            proxy = session.get_file_proxy(file_key)
            
            # åŸºæœ¬åˆ†æ
            basic = proxy.analyzer.analyze()
            
            # å¹³å¦åŒ–
            flattened = proxy.analyzer.apply_flattening(method='polynomial', order=2)
            
            # çµ±è¨ˆæ•¸æ“š
            stats = {
                'original_std': np.std(proxy.image),
                'flattened_std': np.std(flattened['data']['flattened_image']),
                'signal_type': proxy.file_info.signal_type,
                'direction': proxy.file_info.direction
            }
            
            results[file_key] = {
                'basic': basic,
                'flattened': flattened,
                'stats': stats
            }
            
        except Exception as e:
            print(f"åˆ†æ {file_key} å¤±æ•—: {e}")
    
    return results

# ä½¿ç”¨æ‰¹æ¬¡åˆ†æ
topo_results = batch_analyze_topo_files(session, signal_type='Topo')
```

#### è‡ªå‹•å ±å‘Šç”Ÿæˆ
```python
def generate_experiment_report(session):
    """ç”Ÿæˆå¯¦é©—åˆ†æå ±å‘Š"""
    report = {
        'experiment_info': session.experiment_info,
        'scan_parameters': session.scan_parameters,
        'file_summary': {},
        'analysis_results': {}
    }
    
    # æª”æ¡ˆæ‘˜è¦
    for file_type in ['topo', 'cits', 'sts']:
        manager = getattr(session, file_type)
        files = manager.get_files()
        report['file_summary'][file_type] = {
            'count': len(files),
            'total_size': sum(f.size for f in files.values()),
            'signal_types': list(set(f.signal_type for f in files.values() if f.signal_type))
        }
    
    # åˆ†æçµæœæ‘˜è¦
    # ... åˆ†æé‚è¼¯
    
    return report

# ç”Ÿæˆå ±å‘Š
report = generate_experiment_report(session)
```

### è¨˜æ†¶é«”å’Œæ•ˆèƒ½æœ€ä½³åŒ–

#### æ™ºèƒ½å¿«å–ç­–ç•¥
```python
# å¤§æª”æ¡ˆä½¿ç”¨å°å¿«å–
large_file_session = ExperimentSession(cache_size=5)

# å°æª”æ¡ˆä½¿ç”¨å¤§å¿«å–
small_file_session = ExperimentSession(cache_size=50)

# å‹•æ…‹èª¿æ•´å¿«å–å¤§å°
def adjust_cache_size(session):
    memory_info = session.get_memory_info()
    total_size = memory_info['total_loaded_size']
    
    if total_size > 1e9:  # > 1GB
        session.txt.cache_size = 5
        session.topo.cache_size = 5
        session.cits.cache_size = 3
    elif total_size > 5e8:  # > 500MB
        session.txt.cache_size = 10
        session.topo.cache_size = 10
        session.cits.cache_size = 5
```

#### é¸æ“‡æ€§è¼‰å…¥
```python
# åªè¼‰å…¥éœ€è¦çš„æª”æ¡ˆé¡å‹
session = ExperimentSession()
session.load_txt_file(txt_path)

# åªåˆ†æç‰¹å®šè¨Šè™Ÿé¡å‹
topo_files = session.find_files_by_signal_type('Topo')
for file_key in topo_files:
    proxy = session.get_file_proxy(file_key)
    # åªåšå¿…è¦çš„åˆ†æ...

# åŠæ™‚å¸è¼‰ä¸éœ€è¦çš„æª”æ¡ˆ
for file_key in session.topo.get_loaded_files():
    if file_key not in needed_files:
        session.topo.unload(file_key)
```

## ç¯„ä¾‹ç¨‹å¼ç¢¼ / Example Code

### å®Œæ•´åˆ†æå·¥ä½œæµç¨‹ç¯„ä¾‹

```python
"""
KEEN å¾Œç«¯å®Œæ•´åˆ†æå·¥ä½œæµç¨‹ç¯„ä¾‹
Complete analysis workflow example for KEEN backend
"""

from backend.core.experiment_session import ExperimentSession
import numpy as np
import matplotlib.pyplot as plt

def complete_analysis_workflow(txt_file_path):
    """å®Œæ•´çš„ SPM æ•¸æ“šåˆ†æå·¥ä½œæµç¨‹"""
    
    # ============ 1. åˆå§‹åŒ–æœƒè©± ============
    print("ğŸš€ åˆå§‹åŒ–å¯¦é©—æœƒè©±...")
    session = ExperimentSession()
    
    # è¼‰å…¥å¯¦é©—
    success = session.load_txt_file(txt_file_path)
    if not success:
        print("âŒ å¯¦é©—è¼‰å…¥å¤±æ•—")
        return None
    
    print("âœ… å¯¦é©—è¼‰å…¥æˆåŠŸ")
    print(f"ğŸ“ å¯¦é©—åç¨±: {session.experiment_name}")
    
    # ============ 2. æª¢è¦–æª”æ¡ˆæ¦‚æ³ ============
    print("\nğŸ“Š æª”æ¡ˆæ¦‚æ³:")
    available = session.available_files
    for file_type, files in available.items():
        print(f"  {file_type}: {len(files)} å€‹æª”æ¡ˆ")
    
    # ============ 3. æ‹“æ’²åœ–åˆ†æ ============
    print("\nğŸ—ºï¸  é–‹å§‹æ‹“æ’²åœ–åˆ†æ...")
    topo_files = session.get_topo_files()
    
    if topo_files:
        # é¸æ“‡ç¬¬ä¸€å€‹æ‹“æ’²æª”æ¡ˆ
        topo_key = topo_files[0]
        topo = session.get_file_proxy(topo_key)
        
        print(f"   æª”æ¡ˆ: {topo.file_info.filename}")
        print(f"   å°ºå¯¸: {topo.shape}")
        print(f"   ç¯„åœ: {topo.x_range:.1f} x {topo.y_range:.1f} nm")
        
        # åŸºæœ¬åˆ†æ
        print("   åŸ·è¡ŒåŸºæœ¬åˆ†æ...")
        basic_result = topo.analyzer.analyze()
        stats = basic_result['data']['statistics']
        print(f"   é«˜åº¦ç¯„åœ: {stats['min']:.3f} ~ {stats['max']:.3f} nm")
        print(f"   ç²—ç³™åº¦ (RMS): {stats['rms']:.3f} nm")
        
        # å¹³å¦åŒ–è™•ç†
        print("   åŸ·è¡Œå¹³å¦åŒ–è™•ç†...")
        flatten_result = topo.analyzer.apply_flattening(
            method='polynomial',
            order=2
        )
        
        # æå–å‰–é¢ç·š
        print("   æå–å‰–é¢ç·š...")
        profile_result = topo.analyzer.extract_line_profile(
            start_point=(10, topo.shape[0]//2),
            end_point=(topo.shape[1]-10, topo.shape[0]//2)
        )
        
        print(f"   å‰–é¢é•·åº¦: {profile_result['data']['physical_length']:.2f} nm")
        
    # ============ 4. CITS æ•¸æ“šåˆ†æ ============
    print("\nğŸ”¬ é–‹å§‹ CITS æ•¸æ“šåˆ†æ...")
    cits_files = session.get_cits_files()
    
    if cits_files:
        # é¸æ“‡ç¬¬ä¸€å€‹ CITS æª”æ¡ˆ
        cits_key = cits_files[0]
        cits = session.get_file_proxy(cits_key)
        
        print(f"   æª”æ¡ˆ: {cits.file_info.filename}")
        print(f"   æ•¸æ“šå°ºå¯¸: {cits.shape}")
        print(f"   åå£“ç¯„åœ: {cits.bias_values.min():.2f} ~ {cits.bias_values.max():.2f} V")
        
        # åŸºæœ¬åˆ†æ
        print("   åŸ·è¡ŒåŸºæœ¬åˆ†æ...")
        cits_basic = cits.analyzer.analyze()
        pattern = cits_basic['data']['bias_pattern']
        print(f"   åå£“æ¨¡å¼: {pattern['pattern_type']}")
        
        # ç·šæ®µå‰–é¢å…‰è­œ
        print("   æå–ç·šæ®µå‰–é¢å…‰è­œ...")
        line_result = cits.analyzer.extract_line_profile(
            start_coord=(10, 10),
            end_coord=(cits.shape[2]-10, cits.shape[1]-10)
        )
        
        line_sts = line_result['data']['line_sts']
        print(f"   å…‰è­œæ•¸æ“šå½¢ç‹€: {line_sts.shape}")
        
        # åå£“åˆ‡ç‰‡
        print("   ç²å–åå£“åˆ‡ç‰‡...")
        mid_bias = len(cits.bias_values) // 2
        slice_result = cits.analyzer.get_bias_slice(mid_bias)
        print(f"   åå£“ {slice_result['data']['bias_value']:.3f}V åˆ‡ç‰‡å®Œæˆ")
        
    # ============ 5. æ•ˆèƒ½å’Œè¨˜æ†¶é«”ç›£æ§ ============
    print("\nğŸ“ˆ ç³»çµ±ç‹€æ…‹:")
    memory_info = session.get_memory_info()
    print(f"   ç¸½è¼‰å…¥å¤§å°: {memory_info['total_loaded_size']/1e6:.1f} MB")
    print(f"   è¼‰å…¥æª”æ¡ˆæ•¸: {memory_info['total_loaded_files']}")
    
    # å¿«å–æ•ˆèƒ½
    cache_stats = []
    for manager_name in ['txt', 'topo', 'cits', 'sts']:
        manager = getattr(session, manager_name)
        cache_info = manager.get_cache_info()
        cache_stats.append(f"{manager_name}: {cache_info['hit_rate']:.1%}")
    print(f"   å¿«å–å‘½ä¸­ç‡: {', '.join(cache_stats)}")
    
    # ============ 6. ç”Ÿæˆæ‘˜è¦å ±å‘Š ============
    print("\nğŸ“‹ ç”Ÿæˆåˆ†ææ‘˜è¦...")
    summary = session.get_session_summary()
    
    report = {
        'experiment_name': session.experiment_name,
        'total_files': sum(len(files) for files in available.values()),
        'analysis_results': {
            'topo_analyzed': len(topo_files) > 0,
            'cits_analyzed': len(cits_files) > 0,
        },
        'system_performance': {
            'memory_usage_mb': memory_info['total_loaded_size']/1e6,
            'cache_performance': cache_stats
        }
    }
    
    print("âœ… åˆ†æå®Œæˆ!")
    return report

# ============ ä½¿ç”¨ç¯„ä¾‹ ============
if __name__ == "__main__":
    # è¨­å®šå¯¦é©—æª”æ¡ˆè·¯å¾‘
    experiment_path = "/path/to/your/experiment.txt"
    
    # åŸ·è¡Œå®Œæ•´åˆ†æ
    try:
        result = complete_analysis_workflow(experiment_path)
        if result:
            print("\nğŸ‰ åˆ†æå ±å‘Š:")
            for key, value in result.items():
                print(f"  {key}: {value}")
    except Exception as e:
        print(f"âŒ åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
```

### é€²éšè‡ªå‹•åŒ–åˆ†æç¯„ä¾‹

```python
"""
é€²éšè‡ªå‹•åŒ–åˆ†æç¯„ä¾‹ï¼šæ‰¹æ¬¡è™•ç†å¤šå€‹å¯¦é©—
Advanced automation example: batch processing multiple experiments
"""

import os
from pathlib import Path
import pandas as pd

class KeenBatchAnalyzer:
    """KEEN æ‰¹æ¬¡åˆ†æå™¨"""
    
    def __init__(self, cache_size=10):
        self.cache_size = cache_size
        self.results = []
    
    def analyze_experiment_folder(self, folder_path):
        """åˆ†ææ•´å€‹å¯¦é©—è³‡æ–™å¤¾"""
        folder = Path(folder_path)
        txt_files = list(folder.glob("*.txt"))
        
        print(f"ğŸ“ ç™¼ç¾ {len(txt_files)} å€‹å¯¦é©—æª”æ¡ˆ")
        
        for txt_file in txt_files:
            try:
                result = self.analyze_single_experiment(str(txt_file))
                if result:
                    self.results.append(result)
                    print(f"âœ… {txt_file.name} åˆ†æå®Œæˆ")
                else:
                    print(f"âŒ {txt_file.name} åˆ†æå¤±æ•—")
            except Exception as e:
                print(f"âŒ {txt_file.name} éŒ¯èª¤: {e}")
    
    def analyze_single_experiment(self, txt_path):
        """åˆ†æå–®å€‹å¯¦é©—"""
        session = ExperimentSession()
        
        if not session.load_txt_file(txt_path):
            return None
        
        result = {
            'experiment_name': session.experiment_name,
            'txt_path': txt_path,
            'scan_parameters': session.scan_parameters.__dict__,
            'file_counts': {},
            'topo_stats': {},
            'cits_stats': {}
        }
        
        # æª”æ¡ˆçµ±è¨ˆ
        available = session.available_files
        for file_type, files in available.items():
            result['file_counts'][file_type] = len(files)
        
        # æ‹“æ’²åœ–åˆ†æ
        topo_files = session.get_topo_files()
        if topo_files:
            topo_stats = self._analyze_topo_batch(session, topo_files[:3])  # åªåˆ†æå‰3å€‹
            result['topo_stats'] = topo_stats
        
        # CITS åˆ†æ
        cits_files = session.get_cits_files()
        if cits_files:
            cits_stats = self._analyze_cits_batch(session, cits_files[:2])  # åªåˆ†æå‰2å€‹
            result['cits_stats'] = cits_stats
        
        return result
    
    def _analyze_topo_batch(self, session, file_keys):
        """æ‰¹æ¬¡æ‹“æ’²åœ–åˆ†æ"""
        stats = {
            'files_analyzed': len(file_keys),
            'roughness_values': [],
            'height_ranges': [],
            'signal_types': []
        }
        
        for key in file_keys:
            proxy = session.get_file_proxy(key)
            basic_result = proxy.analyzer.analyze()
            
            file_stats = basic_result['data']['statistics']
            stats['roughness_values'].append(file_stats['rms'])
            stats['height_ranges'].append(file_stats['max'] - file_stats['min'])
            stats['signal_types'].append(proxy.file_info.signal_type)
        
        # è¨ˆç®—çµ±è¨ˆæ‘˜è¦
        if stats['roughness_values']:
            stats['avg_roughness'] = np.mean(stats['roughness_values'])
            stats['avg_height_range'] = np.mean(stats['height_ranges'])
        
        return stats
    
    def _analyze_cits_batch(self, session, file_keys):
        """æ‰¹æ¬¡ CITS åˆ†æ"""
        stats = {
            'files_analyzed': len(file_keys),
            'bias_ranges': [],
            'data_sizes': [],
            'bias_patterns': []
        }
        
        for key in file_keys:
            proxy = session.get_file_proxy(key)
            basic_result = proxy.analyzer.analyze()
            
            bias_range = (float(proxy.bias_values.min()), float(proxy.bias_values.max()))
            stats['bias_ranges'].append(bias_range)
            stats['data_sizes'].append(proxy.shape)
            stats['bias_patterns'].append(basic_result['data']['bias_pattern']['pattern_type'])
        
        return stats
    
    def export_results(self, output_path):
        """åŒ¯å‡ºåˆ†æçµæœ"""
        if not self.results:
            print("âŒ æ²’æœ‰åˆ†æçµæœå¯åŒ¯å‡º")
            return
        
        # è½‰æ›ç‚º DataFrame
        rows = []
        for result in self.results:
            row = {
                'experiment_name': result['experiment_name'],
                'txt_path': result['txt_path'],
                'x_pixels': result['scan_parameters']['x_pixel'],
                'y_pixels': result['scan_parameters']['y_pixel'],
                'x_range': result['scan_parameters']['x_range'],
                'y_range': result['scan_parameters']['y_range'],
                'topo_files': result['file_counts'].get('topo', 0),
                'cits_files': result['file_counts'].get('cits', 0),
                'sts_files': result['file_counts'].get('sts', 0),
            }
            
            # æ·»åŠ æ‹“æ’²çµ±è¨ˆ
            if result['topo_stats']:
                row['avg_roughness'] = result['topo_stats'].get('avg_roughness', 0)
                row['avg_height_range'] = result['topo_stats'].get('avg_height_range', 0)
            
            # æ·»åŠ  CITS çµ±è¨ˆ
            if result['cits_stats']:
                row['cits_files_analyzed'] = result['cits_stats']['files_analyzed']
            
            rows.append(row)
        
        # å„²å­˜ CSV
        df = pd.DataFrame(rows)
        df.to_csv(output_path, index=False)
        print(f"ğŸ“Š çµæœå·²åŒ¯å‡ºè‡³: {output_path}")
        
        return df

# ä½¿ç”¨ç¯„ä¾‹
def main():
    # åˆå§‹åŒ–æ‰¹æ¬¡åˆ†æå™¨
    analyzer = KeenBatchAnalyzer(cache_size=5)
    
    # åˆ†æå¯¦é©—è³‡æ–™å¤¾
    experiment_folder = "/path/to/experiments"
    analyzer.analyze_experiment_folder(experiment_folder)
    
    # åŒ¯å‡ºçµæœ
    output_file = "batch_analysis_results.csv"
    df = analyzer.export_results(output_file)
    
    # é¡¯ç¤ºæ‘˜è¦
    if df is not None:
        print(f"\nğŸ“ˆ æ‰¹æ¬¡åˆ†ææ‘˜è¦:")
        print(f"   ç¸½å¯¦é©—æ•¸: {len(df)}")
        print(f"   å¹³å‡æ‹“æ’²æª”æ¡ˆæ•¸: {df['topo_files'].mean():.1f}")
        print(f"   å¹³å‡ CITS æª”æ¡ˆæ•¸: {df['cits_files'].mean():.1f}")
        if 'avg_roughness' in df.columns:
            print(f"   å¹³å‡ç²—ç³™åº¦: {df['avg_roughness'].mean():.3f} nm")

if __name__ == "__main__":
    main()
```

## ç–‘é›£æ’è§£ / Troubleshooting

### å¸¸è¦‹éŒ¯èª¤å’Œè§£æ±ºæ–¹æ¡ˆ

#### 1. æª”æ¡ˆè¼‰å…¥éŒ¯èª¤
```python
# éŒ¯èª¤: FileNotFoundError
try:
    session.load_txt_file("/path/to/file.txt")
except FileNotFoundError:
    print("æª”æ¡ˆä¸å­˜åœ¨ï¼Œè«‹æª¢æŸ¥è·¯å¾‘")

# éŒ¯èª¤: PermissionError  
try:
    session.load_txt_file("/path/to/file.txt")
except PermissionError:
    print("æª”æ¡ˆæ¬Šé™ä¸è¶³ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ¬Šé™")

# æª¢æŸ¥æª”æ¡ˆå­˜åœ¨å’Œæ¬Šé™
import os
file_path = "/path/to/file.txt"
if os.path.exists(file_path):
    if os.access(file_path, os.R_OK):
        print("æª”æ¡ˆå¯è®€å–")
    else:
        print("æª”æ¡ˆæ¬Šé™ä¸è¶³")
else:
    print("æª”æ¡ˆä¸å­˜åœ¨")
```

#### 2. è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤
```python
# ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"è¨˜æ†¶é«”ä½¿ç”¨ç‡: {memory.percent}%")
    print(f"å¯ç”¨è¨˜æ†¶é«”: {memory.available / 1e9:.1f} GB")

# è¨˜æ†¶é«”ä¸è¶³æ™‚çš„è™•ç†ç­–ç•¥
def handle_memory_pressure(session):
    memory_info = session.get_memory_info()
    
    if memory_info['total_loaded_size'] > 2e9:  # > 2GB
        print("è¨˜æ†¶é«”å£“åŠ›å¤§ï¼Œé–‹å§‹æ¸…ç†...")
        
        # æ¸…ç†æœ€å°‘ä½¿ç”¨çš„å¿«å–
        session.clear_all_caches()
        
        # å¸è¼‰éå¿…è¦æª”æ¡ˆ
        for manager_name in ['sts', 'cits', 'topo']:
            manager = getattr(session, manager_name)
            loaded_files = manager.get_loaded_files()
            
            # åªä¿ç•™æœ€è¿‘ä½¿ç”¨çš„æª”æ¡ˆ
            for file_key in loaded_files[5:]:  # ä¿ç•™å‰5å€‹
                manager.unload(file_key)
```

#### 3. åˆ†æå™¨éŒ¯èª¤
```python
# æ•ç²åˆ†æéŒ¯èª¤
def safe_analyze(analyzer, **kwargs):
    try:
        result = analyzer.analyze(**kwargs)
        if not result['success']:
            print(f"åˆ†æå¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            return None
        return result
    except Exception as e:
        print(f"åˆ†æå™¨ç•°å¸¸: {e}")
        return None

# æª¢æŸ¥åˆ†æå™¨ç‹€æ…‹
def check_analyzer_status(analyzer):
    status = analyzer.get_status()
    print(f"åˆ†æå™¨ç‹€æ…‹: {status}")
    
    if status.get('errors'):
        print("éŒ¯èª¤æ­·å²:")
        for error in status['errors'][-3:]:  # æœ€è¿‘3å€‹éŒ¯èª¤
            print(f"  {error['timestamp']}: {error['message']}")
```

#### 4. æ•¸æ“šæ ¼å¼éŒ¯èª¤
```python
# é©—è­‰æ•¸æ“šæ ¼å¼
def validate_data_format(proxy):
    try:
        # æª¢æŸ¥åŸºæœ¬å±¬æ€§
        if proxy.data is None:
            return False, "æ•¸æ“šç‚ºç©º"
        
        # æª¢æŸ¥æ‹“æ’²åœ–æ•¸æ“š
        if proxy.file_type == "topo":
            if proxy.image is None:
                return False, "ç¼ºå°‘åœ–åƒæ•¸æ“š"
            if proxy.image.ndim != 2:
                return False, f"åœ–åƒç¶­åº¦éŒ¯èª¤: {proxy.image.ndim}"
        
        # æª¢æŸ¥ CITS æ•¸æ“š
        elif proxy.file_type == "cits":
            if proxy.data_3d is None:
                return False, "ç¼ºå°‘ 3D æ•¸æ“š"
            if proxy.bias_values is None:
                return False, "ç¼ºå°‘åå£“æ•¸æ“š"
            if proxy.data_3d.shape[0] != len(proxy.bias_values):
                return False, "æ•¸æ“šç¶­åº¦ä¸åŒ¹é…"
        
        return True, "æ•¸æ“šæ ¼å¼æ­£ç¢º"
        
    except Exception as e:
        return False, f"é©—è­‰éç¨‹éŒ¯èª¤: {e}"

# ä½¿ç”¨é©—è­‰
proxy = session.get_file_proxy("some_file")
is_valid, message = validate_data_format(proxy)
print(f"æ•¸æ“šé©—è­‰: {message}")
```

### æ•ˆèƒ½å•é¡Œè¨ºæ–·

#### è¼‰å…¥æ•ˆèƒ½åˆ†æ
```python
import time

def profile_loading_performance(session, file_keys):
    """åˆ†ææª”æ¡ˆè¼‰å…¥æ•ˆèƒ½"""
    results = {}
    
    for file_key in file_keys:
        start_time = time.time()
        
        # è¼‰å…¥æª”æ¡ˆ
        proxy = session.get_file_proxy(file_key)
        load_time = time.time() - start_time
        
        # åˆ†ææª”æ¡ˆ
        start_analysis = time.time()
        result = proxy.analyzer.analyze()
        analysis_time = time.time() - start_analysis
        
        file_size = proxy.file_info.size
        results[file_key] = {
            'load_time': load_time,
            'analysis_time': analysis_time,
            'file_size': file_size,
            'load_speed': file_size / load_time / 1e6,  # MB/s
            'success': result['success']
        }
    
    return results

# ä½¿ç”¨æ•ˆèƒ½åˆ†æ
topo_files = session.get_topo_files()[:5]
perf_results = profile_loading_performance(session, topo_files)

for file_key, stats in perf_results.items():
    print(f"{file_key}:")
    print(f"  è¼‰å…¥æ™‚é–“: {stats['load_time']:.2f}s")
    print(f"  åˆ†ææ™‚é–“: {stats['analysis_time']:.2f}s") 
    print(f"  è¼‰å…¥é€Ÿåº¦: {stats['load_speed']:.1f} MB/s")
```

## æ•ˆèƒ½æœ€ä½³åŒ– / Performance Optimization

### è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥

#### æ™ºèƒ½å¿«å–é…ç½®
```python
def optimize_cache_settings(session, target_memory_gb=4):
    """æ ¹æ“šç›®æ¨™è¨˜æ†¶é«”ä½¿ç”¨é‡æœ€ä½³åŒ–å¿«å–è¨­å®š"""
    target_bytes = target_memory_gb * 1e9
    
    # ä¼°ç®—æ¯å€‹æª”æ¡ˆé¡å‹çš„å¹³å‡å¤§å°
    avg_sizes = {}
    for manager_name in ['txt', 'topo', 'cits', 'sts']:
        manager = getattr(session, manager_name)
        files = manager.get_files()
        
        if files:
            total_size = sum(f.size for f in files.values())
            avg_sizes[manager_name] = total_size / len(files)
        else:
            avg_sizes[manager_name] = 1e6  # é è¨­ 1MB
    
    # æ ¹æ“šæª”æ¡ˆå¤§å°åˆ†é…å¿«å–
    total_avg_size = sum(avg_sizes.values())
    
    for manager_name in ['txt', 'topo', 'cits', 'sts']:
        manager = getattr(session, manager_name)
        
        # æŒ‰æ¯”ä¾‹åˆ†é…è¨˜æ†¶é«”
        allocated_memory = target_bytes * (avg_sizes[manager_name] / total_avg_size)
        cache_size = max(1, int(allocated_memory / avg_sizes[manager_name]))
        
        # è¨­å®šå¿«å–å¤§å°ï¼ˆæœ€å°‘1ï¼Œæœ€å¤š50ï¼‰
        manager._cache_size = min(50, max(1, cache_size))
        
        print(f"{manager_name} å¿«å–å¤§å°: {manager._cache_size}")

# æ‡‰ç”¨æœ€ä½³åŒ–
optimize_cache_settings(session, target_memory_gb=2)
```

#### è¼‰å…¥ç­–ç•¥æœ€ä½³åŒ–
```python
def lazy_loading_strategy(session, analysis_type='basic'):
    """å»¶é²è¼‰å…¥ç­–ç•¥"""
    
    if analysis_type == 'basic':
        # åŸºæœ¬åˆ†æï¼šåªè¼‰å…¥ç¬¬ä¸€å€‹æª”æ¡ˆ
        for file_type in ['topo', 'cits']:
            manager = getattr(session, file_type)
            files = list(manager.get_files().keys())
            if files:
                proxy = session.get_file_proxy(files[0])
                proxy.analyzer.analyze()
                
    elif analysis_type == 'comprehensive':
        # å…¨é¢åˆ†æï¼šæ‰¹æ¬¡è¼‰å…¥ï¼Œç”¨å®Œå³å¸è¼‰
        for file_type in ['topo', 'cits']:
            manager = getattr(session, file_type)
            files = list(manager.get_files().keys())
            
            for i, file_key in enumerate(files):
                proxy = session.get_file_proxy(file_key)
                proxy.analyzer.analyze()
                
                # æ¯è™•ç†5å€‹æª”æ¡ˆï¼Œæ¸…ç†ä¸€æ¬¡å¿«å–
                if i % 5 == 4:
                    manager.clear_cache()
```

### ä¸¦è¡Œè™•ç†

#### å¤šåŸ·è¡Œç·’åˆ†æ
```python
import concurrent.futures
from threading import Lock

class ParallelAnalyzer:
    """ä¸¦è¡Œåˆ†æå™¨"""
    
    def __init__(self, session, max_workers=4):
        self.session = session
        self.max_workers = max_workers
        self.results_lock = Lock()
        self.results = {}
    
    def analyze_files_parallel(self, file_keys, analysis_func):
        """ä¸¦è¡Œåˆ†ææª”æ¡ˆ"""
        
        def worker(file_key):
            try:
                proxy = self.session.get_file_proxy(file_key)
                result = analysis_func(proxy)
                
                with self.results_lock:
                    self.results[file_key] = result
                    
                return file_key, True
                
            except Exception as e:
                print(f"åˆ†æ {file_key} å¤±æ•—: {e}")
                return file_key, False
        
        # ä½¿ç”¨åŸ·è¡Œç·’æ± 
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(worker, key) for key in file_keys]
            
            for future in concurrent.futures.as_completed(futures):
                file_key, success = future.result()
                print(f"{'âœ…' if success else 'âŒ'} {file_key}")
        
        return self.results

# ä½¿ç”¨ä¸¦è¡Œåˆ†æ
def basic_topo_analysis(proxy):
    """åŸºæœ¬æ‹“æ’²åˆ†æå‡½æ•¸"""
    result = proxy.analyzer.analyze()
    flatten_result = proxy.analyzer.apply_flattening(method='polynomial', order=2)
    return {
        'basic': result,
        'flattened': flatten_result
    }

# åŸ·è¡Œä¸¦è¡Œåˆ†æ
parallel_analyzer = ParallelAnalyzer(session, max_workers=3)
topo_files = session.get_topo_files()[:10]  # å‰10å€‹æª”æ¡ˆ
results = parallel_analyzer.analyze_files_parallel(topo_files, basic_topo_analysis)
```

---

**ä½œè€… / Author**: Odindino  
**æœ€å¾Œæ›´æ–° / Last Updated**: 2025-06-07  
**ç‰ˆæœ¬ / Version**: 2.0 Complete Edition

é€™ä»½å®Œæ•´æ‰‹å†Šæä¾›äº† KEEN å¾Œç«¯æ¶æ§‹çš„æ‰€æœ‰è©³ç´°è³‡è¨Šï¼ŒåŒ…æ‹¬å®Œæ•´çš„ API åƒè€ƒã€ä½¿ç”¨æ–¹æ³•ã€é€²éšåŠŸèƒ½å’Œæ•ˆèƒ½æœ€ä½³åŒ–ç­–ç•¥ã€‚å¦‚æœ‰å•é¡Œæˆ–éœ€è¦é€²ä¸€æ­¥èªªæ˜ï¼Œè«‹è¯ç¹«é–‹ç™¼åœ˜éšŠã€‚

For questions or further clarification, please contact the development team.